from llama_cpp import download_model, llama_server

download_model()